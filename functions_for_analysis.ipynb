{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a958fb",
   "metadata": {},
   "source": [
    "This Jupyter notebook contains the functions needed to run the thresholding algorithm\n",
    "\n",
    "External packages: pulp, numpy, pandas, networkx, ghudi, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8794471d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports, declare necessary PATH variables\n",
    "\n",
    "path_to_gurobi = r'C:/gurobi1200/win64' # Set to gurobi path in environment variables\n",
    "import pulp as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gudhi.representations.preprocessing import BirthPersistenceTransform\n",
    "from gudhi.representations.vector_methods import PersistenceImage\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c5a53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lifetimes(hxdx):\n",
    "  \"\"\"\n",
    "  Generate an array of lifetimes as an nx2 array, where for each entry the first value is the birth and the second value is the death\n",
    "\n",
    "  hxdx = (pandas DataFrame) the homology information (we use that information generated by FactoredBoundaryMatrixVr's (oat suite) homology method)\n",
    "          Necessary columns: \"dimension\", \"birth\", \"death\"\n",
    "          Note: x in hxdx acts a placeholder for specific values, i.e. h1d2 (homology of diagram 1 dimension 2)\n",
    "  \"\"\"\n",
    "  hxdx_list = np.column_stack((hxdx.birth.values.tolist(), hxdx.death.values.tolist()))\n",
    "  hxdx_list[:,1][hxdx_list[:,1] == np.inf] = 1.00001 # Replace infinite values with numerical value exceeding the filtration maximum\n",
    "\n",
    "  return hxdx_list\n",
    "\n",
    "\n",
    "\n",
    "def get_euclidean_distance(homology1_path, homology2_path):\n",
    "    \"\"\"\n",
    "    Calculate the euclidean distance between two persistence diagrams by utilizing their vector image representations\n",
    "\n",
    "    homology1_path = (String) path to the first homology csv file\n",
    "    homology2_path = (String) path to the second homology csv file\n",
    "    \"\"\"\n",
    "\n",
    "    # the user can internally add more/different weighting functions\n",
    "\n",
    "    def linear_weight(arr):\n",
    "        \"\"\"\n",
    "        Weight function weighting persistence image linearly along the y-axis\n",
    "        \n",
    "        arr = (np.array) 1x2 array entry of birth persistence transformed persistence diagram\n",
    "        \"\"\"\n",
    "        if type(arr) != type(np.array([])):\n",
    "            raise TypeError(\"arr must have type np.array([])\")\n",
    "        return arr[1]\n",
    "    \n",
    "    bpt = BirthPersistenceTransform()\n",
    "    pi = PersistenceImage(bandwidth=0.1, resolution=[20, 20], weight=linear_weight) # the user may also adjust the bandwidth and resolution of the persistence image\n",
    "    h1 = pd.read_csv(homology1_path)\n",
    "    h2 = pd.read_csv(homology2_path)\n",
    "\n",
    "    # Note that e.g. h1d1 refers to the first array's dimension 1 information\n",
    "    h1d1_lifetimes = get_lifetimes(h1.loc[(h1['dimension'] == 1)])\n",
    "    h1d2_lifetimes = get_lifetimes(h1.loc[(h1['dimension'] == 2)])\n",
    "    h2d1_lifetimes = get_lifetimes(h2.loc[(h2['dimension'] == 1)])\n",
    "    h2d2_lifetimes = get_lifetimes(h2.loc[(h2['dimension'] == 2)])\n",
    "\n",
    "    if np.any(h1d1_lifetimes):\n",
    "        h1d1_transformed = bpt(h1d1_lifetimes)\n",
    "    else:\n",
    "        h1d1_transformed = np.array([[1.0,1.0]]) # Avoid IndexError (empty array)\n",
    "    if np.any(h2d1_lifetimes):\n",
    "        h2d1_transformed = bpt(h2d1_lifetimes)\n",
    "    else:\n",
    "        h2d1_transformed = np.array([[1.0,1.0]]) # Avoid IndexError (empty array)\n",
    "    if np.any(h1d2_lifetimes):\n",
    "        h1d2_transformed = bpt(h1d2_lifetimes)\n",
    "    else:\n",
    "        h1d2_transformed = np.array([[1.0,1.0]]) # Avoid IndexError (empty array)\n",
    "    if np.any(h2d2_lifetimes):\n",
    "        h2d2_transformed = bpt(h2d2_lifetimes)\n",
    "    else:\n",
    "        h2d2_transformed = np.array([[1.0,1.0]]) # Avoid IndexError (empty array)\n",
    "    \n",
    "    h1d1_img = pi(h1d1_transformed)\n",
    "    h2d1_img = pi(h2d1_transformed)\n",
    "    h1d2_img = pi(h1d2_transformed)\n",
    "    h2d2_img = pi(h2d2_transformed)\n",
    "    \n",
    "    h1dx_img = np.concatenate((h1d1_img, h1d2_img))\n",
    "    h2dx_img = np.concatenate((h2d1_img, h2d2_img))\n",
    "    \n",
    "    # Euclidean (p=2) distance is calculated using numpy\n",
    "    temp = np.subtract(h1dx_img, h2dx_img)\n",
    "    return np.sqrt(np.dot(temp.T, temp))\n",
    "\n",
    "\n",
    "\n",
    "def networkSize(path):\n",
    "    \"\"\"\n",
    "    Return the number of nodes (D0 features) in a network\n",
    "\n",
    "    path = (String) path to the homology csv file\n",
    "    \"\"\"\n",
    "    h = pd.read_csv(path)\n",
    "    return(h.loc[(h['dimension'] == 0)].shape[0])\n",
    "\n",
    "\n",
    "\n",
    "def featureCountD1(path):\n",
    "    \"\"\"\n",
    "    Filter a homology csv file by dimension, returning where rows are dimension 1\n",
    "\n",
    "    path = (String) path to the homology csv file\n",
    "    \"\"\"\n",
    "    h = pd.read_csv(path)\n",
    "    return(h.loc[(h['dimension'] == 1)].shape[0])\n",
    "\n",
    "\n",
    "\n",
    "def featureCountD2(path):\n",
    "    \"\"\"\n",
    "    Filter a homology csv file by dimension, returning where rows are dimension 2\n",
    "\n",
    "    path = (String) path to the homology csv file\n",
    "    \"\"\"\n",
    "    h = pd.read_csv(path)\n",
    "    return(h.loc[(h['dimension'] == 2)].shape[0])\n",
    "\n",
    "\n",
    "\n",
    "def fd_gridSearch(folder_path, u, l, normalize=False):\n",
    "    \"\"\"\n",
    "    Generate the average weights and feature counts for each file in the specified folder\n",
    "\n",
    "    folder_path = (String) the folder containing the homology csv files\n",
    "    u = (float) upper bounds vector\n",
    "    l = (float) lower bounds vector\n",
    "    normalize = (Boolean) indicator for whether the feature distrubutions should be normalized by the size of the corresponding network\n",
    "            Note: the feature counts can normalized by the size of the network. This accounts for bias toward larger networks which tend to comprise more higher-dimensional features inherently\n",
    "    \"\"\"\n",
    "    \n",
    "    n_i = len(u) # the code is written intended for the indicial convention (i,j) where i is an upper bound, j is a lower bound. The current data reverses this convention\n",
    "    n_j = len(l)\n",
    "\n",
    "    gradient_dict = {} # in the gradient dictionary, we map elements of the feature space to the approximated magnitude of the gradient on the latent space manifold located at the vectorized feature\n",
    "\n",
    "    # Store separated feature counts\n",
    "    D1_feature_counts = {}\n",
    "    D2_feature_counts = {}\n",
    "\n",
    "    for i in range(n_i):\n",
    "        for j in range(1,n_j): # the data used here includes a lower bound of index 0 which is not considered in the final analysis (i.e. we do not wish to include homology dataframes of the form (0, i).csv); hence it is excluded in the loop\n",
    "\n",
    "            if normalize:\n",
    "                D1_feature_counts[(i,j)] = featureCountD1(folder_path + str((j,i)) + \".csv\")\n",
    "                D2_feature_counts[(i,j)] = featureCountD2(folder_path + str((j,i)) + \".csv\")\n",
    "            else:\n",
    "                D1_feature_counts[(i,j)] = featureCountD1(folder_path + str((j,i)) + \".csv\") / networkSize(folder_path + str((j,i)) + \".csv\") # Normalize, D1\n",
    "                D2_feature_counts[(i,j)] = featureCountD2(folder_path + str((j,i)) + \".csv\") / networkSize(folder_path + str((j,i)) + \".csv\") # Normalize, D2\n",
    "\n",
    "\n",
    "            if i == 0:\n",
    "                if j == 0:\n",
    "\n",
    "                    df_ij_l = get_euclidean_distance(folder_path + str((j+1,i)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    df_ij_u = get_euclidean_distance(folder_path + str((j+1,i)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    dl_ij = np.abs(l[j+1] - l[j])\n",
    "                    du_ij = np.abs(u[i+1] - u[i])\n",
    "\n",
    "                    gradient_dict[(i,j)] = np.array([df_ij_l / dl_ij, df_ij_u / du_ij])\n",
    "\n",
    "                elif j == (n_j - 1):\n",
    "\n",
    "                    df_ij_l = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j-1,i)) + \".csv\")\n",
    "                    df_ij_u = get_euclidean_distance(folder_path + str((j,i+1)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    dl_ij = np.abs(l[j] - l[j-1])\n",
    "                    du_ij = np.abs(u[i+1] - u[i])\n",
    "\n",
    "                    gradient_dict[(i,j)] = np.array([df_ij_l / dl_ij, df_ij_u / du_ij])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    df_ij_l_downstream = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j-1,i)) + \".csv\")\n",
    "                    df_ij_l_upstream = get_euclidean_distance(folder_path + str((j+1,i)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    df_ij_u = get_euclidean_distance(folder_path + str((j,i+1)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    dl_ij_downstream = np.abs(l[j] - l[j-1])\n",
    "                    dl_ij_upstream = np.abs(l[j+1] - l[j])\n",
    "                    du_ij = np.abs(u[i+1] - u[i])\n",
    "\n",
    "                    gradient_dict[(i,j)] = np.array([df_ij_l_downstream / (2*dl_ij_downstream) + df_ij_l_upstream / (2*dl_ij_upstream), df_ij_u / du_ij])\n",
    "\n",
    "            if i == (n_i - 1):\n",
    "\n",
    "                if j == 0:\n",
    "\n",
    "                    df_ij_l = get_euclidean_distance(folder_path + str((j+1,i)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    df_ij_u = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j,i-1)) + \".csv\")\n",
    "                    dl_ij = np.abs(l[j+1] - l[j])\n",
    "                    du_ij = np.abs(u[i] - u[i-1])\n",
    "\n",
    "                    gradient_dict[(i,j)] = np.array([df_ij_l / dl_ij, df_ij_u / du_ij])\n",
    "\n",
    "                elif j == (n_j - 1):\n",
    "\n",
    "                    df_ij_l = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j-1,i)) + \".csv\")\n",
    "                    df_ij_u = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j,i-1)) + \".csv\")\n",
    "                    dl_ij = np.abs(l[j] - l[j-1])\n",
    "                    du_ij = np.abs(u[i] - u[i-1])\n",
    "\n",
    "                    gradient_dict[(i,j)] = np.array([df_ij_l / dl_ij, df_ij_u / du_ij])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    df_ij_l_downstream = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j-1,i)) + \".csv\")\n",
    "                    df_ij_l_upstream = get_euclidean_distance(folder_path + str((j+1,i)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    df_ij_u = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j,i-1)) + \".csv\")\n",
    "                    dl_ij_downstream = np.abs(l[j] - l[j-1])\n",
    "                    dl_ij_upstream = np.abs(l[j+1]-l[j])\n",
    "                    du_ij = np.abs(u[i] - u[i-1])\n",
    "\n",
    "                    gradient_dict[(i,j)] = np.array([df_ij_l_downstream / (2*dl_ij_downstream) + df_ij_l_upstream / (2*dl_ij_upstream), df_ij_u / du_ij])\n",
    "\n",
    "            if (i != 0) and (i != (n_i - 1)):\n",
    "            \n",
    "                if j == 0:\n",
    "\n",
    "                    df_ij_l = get_euclidean_distance(folder_path + str((j+1,i)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    df_ij_u_downstream = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j,i-1)) + \".csv\")\n",
    "                    df_ij_u_upstream = get_euclidean_distance(folder_path + str((j,i+1)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    dl_ij = np.abs(l[j+1] - l[j])\n",
    "                    du_ij_downstream = np.abs(u[i] - u[i-1])\n",
    "                    du_ij_upstream = np.abs(u[i+1]-u[i])\n",
    "\n",
    "                    gradient_dict[(i,j)] = np.array([df_ij_l / dl_ij, df_ij_u_downstream / (2*du_ij_downstream) + df_ij_u_upstream / (2*du_ij_upstream)])\n",
    "            \n",
    "                elif j == (n_j - 1):\n",
    "\n",
    "                    df_ij_l = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j-1,i)) + \".csv\")\n",
    "                    df_ij_u_downstream = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j,i-1)) + \".csv\")\n",
    "                    df_ij_u_upstream = get_euclidean_distance(folder_path + str((j,i+1)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    dl_ij = np.abs(l[j] - l[j-1])\n",
    "                    du_ij_downstream = np.abs(u[i] - u[i-1])\n",
    "                    du_ij_upstream = np.abs(u[i+1]-u[i])\n",
    "\n",
    "                    gradient_dict[(i,j)] = np.array([df_ij_l / dl_ij, df_ij_u_downstream / (2*du_ij_downstream) + df_ij_u_upstream / (2*du_ij_upstream)])\n",
    "\n",
    "                else:\n",
    "\n",
    "                    df_ij_l_downstream = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j-1,i)) + \".csv\")\n",
    "                    df_ij_l_upstream = get_euclidean_distance(folder_path + str((j+1,i)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    df_ij_u_downstream = get_euclidean_distance(folder_path + str((j,i)) + \".csv\", folder_path + str((j,i-1)) + \".csv\")\n",
    "                    df_ij_u_upstream = get_euclidean_distance(folder_path + str((j,i+1)) + \".csv\", folder_path + str((j,i)) + \".csv\")\n",
    "                    dl_ij_downstream = np.abs(l[j] - l[j-1])\n",
    "                    dl_ij_upstream = np.abs(l[j+1]-l[j])\n",
    "                    du_ij_downstream = np.abs(u[i] - u[i-1])\n",
    "                    du_ij_upstream = np.abs(u[i+1] - u[i])\n",
    "\n",
    "                    gradient_dict[(i,j)] = np.array([df_ij_l_downstream / (2*dl_ij_downstream) + df_ij_l_upstream / (2*dl_ij_upstream), df_ij_u_downstream / (2*du_ij_downstream) + df_ij_u_upstream / (2*du_ij_upstream)])\n",
    "\n",
    "    return gradient_dict, D1_feature_counts, D2_feature_counts\n",
    "\n",
    "\n",
    "\n",
    "def optimal_threshold(df_dx, features_d1, features_d2, n, delta_ratio_d1, delta_ratio_d2, quantile=False, solver_path=path_to_gurobi):\n",
    "    \"\"\"\n",
    "    Calculates the optimal persistence diagram by solving the following linear program:\n",
    "    Minimize: W_0*X_0 + ... + W_N*X_N\n",
    "    Subject to: X_0 + ... + X_N = 1 (Decision variables)\n",
    "                Fk_0*X_0 + ... + Fk_N*X_N >= delta_k for all k=1,...,(maximum dimension)\n",
    "    Where: W_i = average weight to neighbors\n",
    "           Fk_i = number of dimension k features per persistence diagram i\n",
    "           delta_k = delta_ratio * max(Fk_i)\n",
    "\n",
    "    When run in a Jupyter notebook, the optimal decision variable is printed in the cell output. The code can be modified to print the decision variable.\n",
    "\n",
    "    avg_weights = (arr_like) sum(w(i,k)/deg(i)) where k are the neighbors of node i, for all nodes i, must have same order as avg_weights\n",
    "    features_d1 = (arr_like) feature counts of persistence diagrams, must have same order as avg_weights (D1)\n",
    "    features_d2 = (arr_like) feature counts of persistence diagrams, must have same order as avg_weights (D2)\n",
    "    n = number of persistence diagrams\n",
    "    delta_ratio = (float) the percentage of the max to use as the delta OR percentile (quantile)\n",
    "    quantile = (Boolean) default False, must be set True if deriving delta_ratio per a quantile\n",
    "    solver_path = (String) environment variable for desired solver, default = Gurobi\n",
    "    \"\"\"\n",
    "\n",
    "    # Formatting\n",
    "    if len(features_d1) != len(df_dx) and len(features_d1) != n:\n",
    "        raise ValueError(\"Length of dimension one features list must match length of gradients list\")\n",
    "    if len(features_d2) != len(df_dx) and len(features_d2) != n:\n",
    "        raise ValueError(\"Length of dimension two features list must match length of gradients list\")\n",
    "    f_d1 = np.asarray(features_d1)\n",
    "    f_d2 = np.asarray(features_d2)\n",
    "    w = np.asarray(df_dx)\n",
    "    \n",
    "    if quantile:\n",
    "        delta_d1 = np.percentile(f_d1, delta_ratio_d1)\n",
    "        delta_d2 = np.percentile(f_d2, delta_ratio_d2)\n",
    "    else:\n",
    "        delta_d1 = delta_ratio_d1 * f_d1.max()\n",
    "        delta_d2 = delta_ratio_d2 * f_d2.max()\n",
    "\n",
    "    # Configure the solver\n",
    "    solver = pl.GUROBI(solver_path, msg=0)\n",
    "\n",
    "    # Create the problem\n",
    "    prob = pl.LpProblem(\"Grid Search\", pl.LpMinimize)\n",
    "\n",
    "    # Establish the variables\n",
    "    x = [pl.LpVariable(\"Chi_\" + str(i),0,1,pl.LpInteger) for i in range(n)]\n",
    "    x = np.asarray(x)\n",
    "\n",
    "    # Add the objective function to the problem first\n",
    "    prob += np.matmul(w.T, x), \"Average Distance To Neighbors\"\n",
    "\n",
    "    # Add the constraints second to the objective\n",
    "    prob += np.sum(x) == 1, \"Single Decision Requirement\"\n",
    "    prob += np.matmul(f_d1.T, x) >= delta_d1, \"Delta Requirement (D1)\"\n",
    "    prob += np.matmul(f_d2.T, x) >= delta_d2, \"Delta Requirement (D2)\"\n",
    "\n",
    "    # Solve (indicate the solver)\n",
    "    prob.solve(solver)\n",
    "    # print(\"Status:\", pl.LpStatus[prob.status]) # uncomment to print status\n",
    "    for v in prob.variables():\n",
    "        if v.varValue == 1.0:\n",
    "            # print(\"Optimal Index: \" + v.name, \"=\", v.varValue) # The nonzero decison variable indicates the index for the PD with the optimal threshold\n",
    "            return v.name\n",
    "        \n",
    "\n",
    "\n",
    "def extract_numbers(text):\n",
    "  \"\"\"\n",
    "  Extract numbers from a string. Function taken from Google AI response\n",
    "\n",
    "  text: (String) text from which to extract a number\n",
    "  \"\"\"\n",
    "  return re.findall(r'\\d+', text)\n",
    "\n",
    "\n",
    "\n",
    "def get_indices(num, upper_list, lower_list):\n",
    "    \"\"\"\n",
    "    Find the indices (on the 'grid' of networks) corresponding to a given decision variable\n",
    "\n",
    "    num = (String) a decision variable given in the form of a string (e.g. \"Chi_10\")\n",
    "    \"\"\"\n",
    "    num = int(extract_numbers(num)[0])\n",
    "    ct = 0\n",
    "    for i in range(len(upper_list)):\n",
    "        for j in range(1,len(lower_list)):\n",
    "            if ct == num:\n",
    "                return([i,j])\n",
    "            ct += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f9b24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oat_env",
   "language": "python",
   "name": "oat_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
